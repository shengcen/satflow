>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002666741581794691,
>>> current period 2,
>,-0.0002912872451249159,
episode 0: ,
overall reward tensor(-0.1214),
overall switching reward tensor(-0.1205),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002403289770995909,
>>> current period 2,
>,-0.0002748313319268891,
episode 1: ,
overall reward tensor(-0.1161),
overall switching reward tensor(-0.1153),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002616934003879647,
>>> current period 2,
>,-0.0002710697485945259,
episode 2: ,
overall reward tensor(-0.1178),
overall switching reward tensor(-0.1169),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002805310749837734,
>>> current period 2,
>,-0.00029583882817201565,
episode 3: ,
overall reward tensor(-0.1096),
overall switching reward tensor(-0.1088),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002875855685892696,
>>> current period 2,
>,-0.00034148901954789916,
episode 4: ,
overall reward tensor(-0.1054),
overall switching reward tensor(-0.1045),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000292703543734018,
>>> current period 2,
>,-0.00028502150327527534,
episode 5: ,
overall reward tensor(-0.1126),
overall switching reward tensor(-0.1117),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00029299797616457075,
>>> current period 2,
>,-0.00024527827482828245,
episode 6: ,
overall reward tensor(-0.1087),
overall switching reward tensor(-0.1078),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000245573226106099,
>>> current period 2,
>,-0.00021286356779899707,
episode 7: ,
overall reward tensor(-0.1061),
overall switching reward tensor(-0.1054),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000325490637433787,
>>> current period 2,
>,-0.00020671234319614724,
episode 8: ,
overall reward tensor(-0.1043),
overall switching reward tensor(-0.1035),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026778101657900496,
>>> current period 2,
>,-0.0002159789391746755,
episode 9: ,
overall reward tensor(-0.1105),
overall switching reward tensor(-0.1097),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00024772340890632946,
>>> current period 2,
>,-0.00029008595209337125,
episode 10: ,
overall reward tensor(-0.1004),
overall switching reward tensor(-0.0996),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002558182072628305,
>>> current period 2,
>,-0.0002454787241967806,
episode 11: ,
overall reward tensor(-0.1112),
overall switching reward tensor(-0.1104),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003139153232563447,
>>> current period 2,
>,-0.0002304945992023573,
episode 12: ,
overall reward tensor(-0.1162),
overall switching reward tensor(-0.1154),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00022385260305039865,
>>> current period 2,
>,-0.0002245187112576975,
episode 13: ,
overall reward tensor(-0.1115),
overall switching reward tensor(-0.1108),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00023872684927015708,
>>> current period 2,
>,-0.0002649546442394999,
episode 14: ,
overall reward tensor(-0.1026),
overall switching reward tensor(-0.1019),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002881773144924282,
>>> current period 2,
>,-0.00027086748878139393,
episode 15: ,
overall reward tensor(-0.1152),
overall switching reward tensor(-0.1144),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00025858482434371196,
>>> current period 2,
>,-0.00029867155770628017,
episode 16: ,
overall reward tensor(-0.1135),
overall switching reward tensor(-0.1127),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00022563142137136446,
>>> current period 2,
>,-0.0002618419843214166,
episode 17: ,
overall reward tensor(-0.1026),
overall switching reward tensor(-0.1019),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000250344351520808,
>>> current period 2,
>,-0.00022245457240683503,
episode 18: ,
overall reward tensor(-0.1072),
overall switching reward tensor(-0.1065),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002657550327913634,
>>> current period 2,
>,-0.00026134075051140063,
episode 19: ,
overall reward tensor(-0.1027),
overall switching reward tensor(-0.1019),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00025008489736027054,
>>> current period 2,
>,-0.00028347119596627927,
episode 20: ,
overall reward tensor(-0.0990),
overall switching reward tensor(-0.0982),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00030458651318762957,
>>> current period 2,
>,-0.0002732878952005823,
episode 21: ,
overall reward tensor(-0.1043),
overall switching reward tensor(-0.1034),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00025603412210141625,
>>> current period 2,
>,-0.000284154126897098,
episode 22: ,
overall reward tensor(-0.1051),
overall switching reward tensor(-0.1043),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002855775787749948,
>>> current period 2,
>,-0.0002850140640515511,
episode 23: ,
overall reward tensor(-0.0901),
overall switching reward tensor(-0.0892),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00022775934664643789,
>>> current period 2,
>,-0.0002769137156221627,
episode 24: ,
overall reward tensor(-0.0961),
overall switching reward tensor(-0.0953),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00030751875759755485,
>>> current period 2,
>,-0.00030602312776082427,
episode 25: ,
overall reward tensor(-0.1002),
overall switching reward tensor(-0.0993),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00025363020411677326,
>>> current period 2,
>,-0.00023988511999525184,
episode 26: ,
overall reward tensor(-0.0833),
overall switching reward tensor(-0.0825),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003486390272803252,
>>> current period 2,
>,-0.00025628619771430824,
episode 27: ,
overall reward tensor(-0.0959),
overall switching reward tensor(-0.0950),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00031684985340135166,
>>> current period 2,
>,-0.0002968872600543788,
episode 28: ,
overall reward tensor(-0.0997),
overall switching reward tensor(-0.0988),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00036323595911630455,
>>> current period 2,
>,-0.00026520440327574646,
episode 29: ,
overall reward tensor(-0.0923),
overall switching reward tensor(-0.0914),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00030546679518100975,
>>> current period 2,
>,-0.0003149080903797269,
episode 30: ,
overall reward tensor(-0.0893),
overall switching reward tensor(-0.0884),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00023635422831224276,
>>> current period 2,
>,-0.00030616286012186524,
episode 31: ,
overall reward tensor(-0.0786),
overall switching reward tensor(-0.0777),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027145795706160023,
>>> current period 2,
>,-0.0003243898584901519,
episode 32: ,
overall reward tensor(-0.0999),
overall switching reward tensor(-0.0991),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002900303473321944,
>>> current period 2,
>,-0.00029045015648787223,
episode 33: ,
overall reward tensor(-0.0959),
overall switching reward tensor(-0.0950),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002768117138277876,
>>> current period 2,
>,-0.00032537414225381346,
episode 34: ,
overall reward tensor(-0.0931),
overall switching reward tensor(-0.0922),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00024137239435365117,
>>> current period 2,
>,-0.00022764838250759857,
episode 35: ,
overall reward tensor(-0.0973),
overall switching reward tensor(-0.0965),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026505896783790615,
>>> current period 2,
>,-0.00031315453343048246,
episode 36: ,
overall reward tensor(-0.0905),
overall switching reward tensor(-0.0896),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00025502077900227757,
>>> current period 2,
>,-0.00026949226069348327,
episode 37: ,
overall reward tensor(-0.0857),
overall switching reward tensor(-0.0849),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00023329790303377098,
>>> current period 2,
>,-0.00031856355012769633,
episode 38: ,
overall reward tensor(-0.0977),
overall switching reward tensor(-0.0969),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00021716059090348458,
>>> current period 2,
>,-0.0003316145562453836,
episode 39: ,
overall reward tensor(-0.1032),
overall switching reward tensor(-0.1023),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00023653823106388325,
>>> current period 2,
>,-0.00030118636616134923,
episode 40: ,
overall reward tensor(-0.1015),
overall switching reward tensor(-0.1006),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002689532799559908,
>>> current period 2,
>,-0.000323482559963261,
episode 41: ,
overall reward tensor(-0.1028),
overall switching reward tensor(-0.1019),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002794872743564985,
>>> current period 2,
>,-0.0002505355095811533,
episode 42: ,
overall reward tensor(-0.0828),
overall switching reward tensor(-0.0820),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002789535199733164,
>>> current period 2,
>,-0.0003102248503428683,
episode 43: ,
overall reward tensor(-0.0887),
overall switching reward tensor(-0.0878),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00020875991236375972,
>>> current period 2,
>,-0.00023945112538601118,
episode 44: ,
overall reward tensor(-0.0851),
overall switching reward tensor(-0.0844),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003080314638149927,
>>> current period 2,
>,-0.0003099346754811378,
episode 45: ,
overall reward tensor(-0.0946),
overall switching reward tensor(-0.0937),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002739416007697156,
>>> current period 2,
>,-0.00028587862142434264,
episode 46: ,
overall reward tensor(-0.0816),
overall switching reward tensor(-0.0807),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002728399441781444,
>>> current period 2,
>,-0.00030665489132069277,
episode 47: ,
overall reward tensor(-0.0829),
overall switching reward tensor(-0.0820),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000278566266170746,
>>> current period 2,
>,-0.000236627366595459,
episode 48: ,
overall reward tensor(-0.0788),
overall switching reward tensor(-0.0780),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027530973571523006,
>>> current period 2,
>,-0.00026387010514429077,
episode 49: ,
overall reward tensor(-0.1061),
overall switching reward tensor(-0.1053),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026896777247493797,
>>> current period 2,
>,-0.00027515130284571894,
episode 50: ,
overall reward tensor(-0.0987),
overall switching reward tensor(-0.0978),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00022746371000398148,
>>> current period 2,
>,-0.0002298087039499045,
episode 51: ,
overall reward tensor(-0.1070),
overall switching reward tensor(-0.1062),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027707963638055514,
>>> current period 2,
>,-0.0002984552931968341,
episode 52: ,
overall reward tensor(-0.0980),
overall switching reward tensor(-0.0972),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002826030962878709,
>>> current period 2,
>,-0.0002899452181557792,
episode 53: ,
overall reward tensor(-0.0856),
overall switching reward tensor(-0.0847),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00030016715518366594,
>>> current period 2,
>,-0.00029668723338230395,
episode 54: ,
overall reward tensor(-0.0841),
overall switching reward tensor(-0.0832),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003168419122739749,
>>> current period 2,
>,-0.00025896084541758574,
episode 55: ,
overall reward tensor(-0.1035),
overall switching reward tensor(-0.1026),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002849974106172545,
>>> current period 2,
>,-0.00025285050630309075,
episode 56: ,
overall reward tensor(-0.0920),
overall switching reward tensor(-0.0912),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026601593621534823,
>>> current period 2,
>,-0.0002102008037197939,
episode 57: ,
overall reward tensor(-0.0923),
overall switching reward tensor(-0.0916),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003142094164470656,
>>> current period 2,
>,-0.00022751236739673768,
episode 58: ,
overall reward tensor(-0.0865),
overall switching reward tensor(-0.0857),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003047708887796404,
>>> current period 2,
>,-0.0002488874068224829,
episode 59: ,
overall reward tensor(-0.0926),
overall switching reward tensor(-0.0917),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003835641759039253,
>>> current period 2,
>,-0.00033287100821797864,
episode 60: ,
overall reward tensor(-0.0924),
overall switching reward tensor(-0.0914),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00032631579203514134,
>>> current period 2,
>,-0.00022791048656961569,
episode 61: ,
overall reward tensor(-0.1034),
overall switching reward tensor(-0.1026),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000295872167426139,
>>> current period 2,
>,-0.00029527873614274767,
episode 62: ,
overall reward tensor(-0.0950),
overall switching reward tensor(-0.0942),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000338511002004049,
>>> current period 2,
>,-0.000310867059369802,
episode 63: ,
overall reward tensor(-0.0958),
overall switching reward tensor(-0.0948),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003581014334515383,
>>> current period 2,
>,-0.00032788191067578074,
episode 64: ,
overall reward tensor(-0.1071),
overall switching reward tensor(-0.1061),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002890666351110102,
>>> current period 2,
>,-0.0003013126325991275,
episode 65: ,
overall reward tensor(-0.1071),
overall switching reward tensor(-0.1062),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000292888790366484,
>>> current period 2,
>,-0.0002633756655074191,
episode 66: ,
overall reward tensor(-0.0921),
overall switching reward tensor(-0.0913),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002620156735977447,
>>> current period 2,
>,-0.0002817834433530828,
episode 67: ,
overall reward tensor(-0.0991),
overall switching reward tensor(-0.0983),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002610352159758142,
>>> current period 2,
>,-0.0003175448611015911,
episode 68: ,
overall reward tensor(-0.1087),
overall switching reward tensor(-0.1078),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00037328513202105804,
>>> current period 2,
>,-0.00025938049948132666,
episode 69: ,
overall reward tensor(-0.1095),
overall switching reward tensor(-0.1086),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002726734223199459,
>>> current period 2,
>,-0.00029581477593895603,
episode 70: ,
overall reward tensor(-0.0874),
overall switching reward tensor(-0.0865),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002826654772776737,
>>> current period 2,
>,-0.00028628414914641027,
episode 71: ,
overall reward tensor(-0.1152),
overall switching reward tensor(-0.1143),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002804096903695582,
>>> current period 2,
>,-0.00027672244989541966,
episode 72: ,
overall reward tensor(-0.0974),
overall switching reward tensor(-0.0966),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000256981419523605,
>>> current period 2,
>,-0.00023875828837138948,
episode 73: ,
overall reward tensor(-0.0949),
overall switching reward tensor(-0.0941),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027766779664675946,
>>> current period 2,
>,-0.0002963324258687481,
episode 74: ,
overall reward tensor(-0.0993),
overall switching reward tensor(-0.0984),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002936532846657415,
>>> current period 2,
>,-0.0002786574355869075,
episode 75: ,
overall reward tensor(-0.1056),
overall switching reward tensor(-0.1048),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003200157265130105,
>>> current period 2,
>,-0.00025891744515317414,
episode 76: ,
overall reward tensor(-0.1063),
overall switching reward tensor(-0.1054),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028519285516525727,
>>> current period 2,
>,-0.000235887561255836,
episode 77: ,
overall reward tensor(-0.0966),
overall switching reward tensor(-0.0958),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00034102337779001545,
>>> current period 2,
>,-0.00026969748749935697,
episode 78: ,
overall reward tensor(-0.1059),
overall switching reward tensor(-0.1050),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000321264679128561,
>>> current period 2,
>,-0.00028642986924175696,
episode 79: ,
overall reward tensor(-0.0992),
overall switching reward tensor(-0.0983),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00023319632137064163,
>>> current period 2,
>,-0.0003203398691639241,
episode 80: ,
overall reward tensor(-0.1068),
overall switching reward tensor(-0.1060),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00029579088587735345,
>>> current period 2,
>,-0.0002316125072367249,
episode 81: ,
overall reward tensor(-0.1034),
overall switching reward tensor(-0.1025),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003048666427635497,
>>> current period 2,
>,-0.00028351583934163084,
episode 82: ,
overall reward tensor(-0.0933),
overall switching reward tensor(-0.0924),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00031149676755557913,
>>> current period 2,
>,-0.00026180642346087954,
episode 83: ,
overall reward tensor(-0.1110),
overall switching reward tensor(-0.1101),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003485361178136513,
>>> current period 2,
>,-0.00031351747101889357,
episode 84: ,
overall reward tensor(-0.1041),
overall switching reward tensor(-0.1032),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027032659590806767,
>>> current period 2,
>,-0.00024205642931481735,
episode 85: ,
overall reward tensor(-0.1052),
overall switching reward tensor(-0.1044),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002551303408112597,
>>> current period 2,
>,-0.00030552078605971004,
episode 86: ,
overall reward tensor(-0.0971),
overall switching reward tensor(-0.0962),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003159641017584599,
>>> current period 2,
>,-0.0002728669695765616,
episode 87: ,
overall reward tensor(-0.0911),
overall switching reward tensor(-0.0902),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002873348980756682,
>>> current period 2,
>,-0.0002854469144578909,
episode 88: ,
overall reward tensor(-0.1062),
overall switching reward tensor(-0.1053),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027792159461755933,
>>> current period 2,
>,-0.0002637960928250146,
episode 89: ,
overall reward tensor(-0.0903),
overall switching reward tensor(-0.0895),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027686142808747486,
>>> current period 2,
>,-0.00041625253169565494,
episode 90: ,
overall reward tensor(-0.1187),
overall switching reward tensor(-0.1177),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002225677477635115,
>>> current period 2,
>,-0.00028717743214099487,
episode 91: ,
overall reward tensor(-0.0962),
overall switching reward tensor(-0.0954),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003344126714078895,
>>> current period 2,
>,-0.0002559869249674675,
episode 92: ,
overall reward tensor(-0.0915),
overall switching reward tensor(-0.0907),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00021998369002119024,
>>> current period 2,
>,-0.00024395932838369423,
episode 93: ,
overall reward tensor(-0.0947),
overall switching reward tensor(-0.0940),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028390180601799793,
>>> current period 2,
>,-0.00025785305478991815,
episode 94: ,
overall reward tensor(-0.1027),
overall switching reward tensor(-0.1019),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000270588038680697,
>>> current period 2,
>,-0.0002961302247605933,
episode 95: ,
overall reward tensor(-0.0993),
overall switching reward tensor(-0.0984),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002324121268330998,
>>> current period 2,
>,-0.0002250478514805371,
episode 96: ,
overall reward tensor(-0.0795),
overall switching reward tensor(-0.0787),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00016705361445041588,
>>> current period 2,
>,-0.0002739619008842372,
episode 97: ,
overall reward tensor(-0.0890),
overall switching reward tensor(-0.0883),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002547887841970401,
>>> current period 2,
>,-0.0002949502311072171,
episode 98: ,
overall reward tensor(-0.1024),
overall switching reward tensor(-0.1016),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00038017641016428033,
>>> current period 2,
>,-0.0002900368005894921,
episode 99: ,
overall reward tensor(-0.1034),
overall switching reward tensor(-0.1025),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00031292391894735734,
>>> current period 2,
>,-0.0002927221805799683,
episode 100: ,
overall reward tensor(-0.1019),
overall switching reward tensor(-0.1010),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027052425118202067,
>>> current period 2,
>,-0.00034809833948747483,
episode 101: ,
overall reward tensor(-0.1082),
overall switching reward tensor(-0.1073),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002592590593049487,
>>> current period 2,
>,-0.0002914659924960445,
episode 102: ,
overall reward tensor(-0.0990),
overall switching reward tensor(-0.0982),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003699024337617376,
>>> current period 2,
>,-0.00029334255212885054,
episode 103: ,
overall reward tensor(-0.1102),
overall switching reward tensor(-0.1092),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00031233192093020807,
>>> current period 2,
>,-0.00028013500651551175,
episode 104: ,
overall reward tensor(-0.0967),
overall switching reward tensor(-0.0958),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003458198623481696,
>>> current period 2,
>,-0.0003103219093983038,
episode 105: ,
overall reward tensor(-0.0942),
overall switching reward tensor(-0.0933),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026380080370088266,
>>> current period 2,
>,-0.00024639425350001426,
episode 106: ,
overall reward tensor(-0.0990),
overall switching reward tensor(-0.0982),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00021132357714168015,
>>> current period 2,
>,-0.0002828930586541931,
episode 107: ,
overall reward tensor(-0.0842),
overall switching reward tensor(-0.0835),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028978227965914966,
>>> current period 2,
>,-0.0002749881163753527,
episode 108: ,
overall reward tensor(-0.0959),
overall switching reward tensor(-0.0951),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002449435411395139,
>>> current period 2,
>,-0.00028459412118042593,
episode 109: ,
overall reward tensor(-0.0944),
overall switching reward tensor(-0.0936),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00020516533804147306,
>>> current period 2,
>,-0.00026149155652092606,
episode 110: ,
overall reward tensor(-0.1022),
overall switching reward tensor(-0.1015),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002882207007041207,
>>> current period 2,
>,-0.00029642832111121103,
episode 111: ,
overall reward tensor(-0.0924),
overall switching reward tensor(-0.0915),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026672395470875445,
>>> current period 2,
>,-0.000312325977299345,
episode 112: ,
overall reward tensor(-0.1111),
overall switching reward tensor(-0.1103),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002477372733663035,
>>> current period 2,
>,-0.00026480224522530086,
episode 113: ,
overall reward tensor(-0.0944),
overall switching reward tensor(-0.0936),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00021758349389070522,
>>> current period 2,
>,-0.0002402299401847476,
episode 114: ,
overall reward tensor(-0.0994),
overall switching reward tensor(-0.0987),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00022990441083891555,
>>> current period 2,
>,-0.0002633690910309156,
episode 115: ,
overall reward tensor(-0.0987),
overall switching reward tensor(-0.0979),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00015131341206349775,
>>> current period 2,
>,-0.000236552114129515,
episode 116: ,
overall reward tensor(-0.0812),
overall switching reward tensor(-0.0805),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002811681718407647,
>>> current period 2,
>,-0.00025578808332438945,
episode 117: ,
overall reward tensor(-0.0883),
overall switching reward tensor(-0.0874),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00024933908917785624,
>>> current period 2,
>,-0.00027615039970661266,
episode 118: ,
overall reward tensor(-0.0827),
overall switching reward tensor(-0.0819),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00019166885331314013,
>>> current period 2,
>,-0.0002924448166726763,
episode 119: ,
overall reward tensor(-0.0926),
overall switching reward tensor(-0.0919),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002754629971214126,
>>> current period 2,
>,-0.00023212129903669545,
episode 120: ,
overall reward tensor(-0.0890),
overall switching reward tensor(-0.0882),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028714148158310367,
>>> current period 2,
>,-0.00029339604080560176,
episode 121: ,
overall reward tensor(-0.0934),
overall switching reward tensor(-0.0925),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002697321502890115,
>>> current period 2,
>,-0.0002566120416888361,
episode 122: ,
overall reward tensor(-0.0833),
overall switching reward tensor(-0.0825),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00021500161232710068,
>>> current period 2,
>,-0.00030152337264135396,
episode 123: ,
overall reward tensor(-0.0900),
overall switching reward tensor(-0.0892),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00021039203513754432,
>>> current period 2,
>,-0.00024205772726123268,
episode 124: ,
overall reward tensor(-0.0976),
overall switching reward tensor(-0.0968),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002870210283080168,
>>> current period 2,
>,-0.0002715208092007548,
episode 125: ,
overall reward tensor(-0.1028),
overall switching reward tensor(-0.1020),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026886462260323823,
>>> current period 2,
>,-0.0002448489980767509,
episode 126: ,
overall reward tensor(-0.0915),
overall switching reward tensor(-0.0907),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026554511054779017,
>>> current period 2,
>,-0.0002744435411928878,
episode 127: ,
overall reward tensor(-0.0870),
overall switching reward tensor(-0.0862),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002767982062678546,
>>> current period 2,
>,-0.0002292982417586367,
episode 128: ,
overall reward tensor(-0.0838),
overall switching reward tensor(-0.0830),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00020835383908767815,
>>> current period 2,
>,-0.0002998985647112873,
episode 129: ,
overall reward tensor(-0.0888),
overall switching reward tensor(-0.0880),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028342205318473903,
>>> current period 2,
>,-0.00027210342195642856,
episode 130: ,
overall reward tensor(-0.0941),
overall switching reward tensor(-0.0932),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0004103395815466332,
>>> current period 2,
>,-0.0002743314911318908,
episode 131: ,
overall reward tensor(-0.1188),
overall switching reward tensor(-0.1178),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00024820545543062994,
>>> current period 2,
>,-0.00026777201736733,
episode 132: ,
overall reward tensor(-0.1015),
overall switching reward tensor(-0.1007),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002857369313622055,
>>> current period 2,
>,-0.00019999588962087681,
episode 133: ,
overall reward tensor(-0.0965),
overall switching reward tensor(-0.0958),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027490091492738213,
>>> current period 2,
>,-0.000270365250443203,
episode 134: ,
overall reward tensor(-0.0897),
overall switching reward tensor(-0.0888),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002541714893645322,
>>> current period 2,
>,-0.0002494212115326952,
episode 135: ,
overall reward tensor(-0.0961),
overall switching reward tensor(-0.0953),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028612172430037756,
>>> current period 2,
>,-0.0003348421912771792,
episode 136: ,
overall reward tensor(-0.1034),
overall switching reward tensor(-0.1025),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002758648799093623,
>>> current period 2,
>,-0.000261990575462864,
episode 137: ,
overall reward tensor(-0.0869),
overall switching reward tensor(-0.0861),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002931432791505892,
>>> current period 2,
>,-0.0002540696872957058,
episode 138: ,
overall reward tensor(-0.0956),
overall switching reward tensor(-0.0948),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00031065223336222274,
>>> current period 2,
>,-0.0002615171878831775,
episode 139: ,
overall reward tensor(-0.1032),
overall switching reward tensor(-0.1023),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026246738081509496,
>>> current period 2,
>,-0.00022795740004445791,
episode 140: ,
overall reward tensor(-0.0955),
overall switching reward tensor(-0.0947),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002444075086624306,
>>> current period 2,
>,-0.0002544671496047174,
episode 141: ,
overall reward tensor(-0.0915),
overall switching reward tensor(-0.0907),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026840351826533967,
>>> current period 2,
>,-0.00027716494006844306,
episode 142: ,
overall reward tensor(-0.0917),
overall switching reward tensor(-0.0909),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00030285689023453946,
>>> current period 2,
>,-0.0002744392632737983,
episode 143: ,
overall reward tensor(-0.0953),
overall switching reward tensor(-0.0945),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00021341352068001006,
>>> current period 2,
>,-0.00026837456091496664,
episode 144: ,
overall reward tensor(-0.0817),
overall switching reward tensor(-0.0809),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002852598889349971,
>>> current period 2,
>,-0.00025833772207571197,
episode 145: ,
overall reward tensor(-0.0956),
overall switching reward tensor(-0.0947),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00031879088949182487,
>>> current period 2,
>,-0.000311805191962383,
episode 146: ,
overall reward tensor(-0.0961),
overall switching reward tensor(-0.0951),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00022465958094564128,
>>> current period 2,
>,-0.00020346795543913925,
episode 147: ,
overall reward tensor(-0.0876),
overall switching reward tensor(-0.0868),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002454777075799076,
>>> current period 2,
>,-0.0002414803642989397,
episode 148: ,
overall reward tensor(-0.0954),
overall switching reward tensor(-0.0946),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00033594092665678604,
>>> current period 2,
>,-0.0002905231053633172,
episode 149: ,
overall reward tensor(-0.1034),
overall switching reward tensor(-0.1024),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027405840310670355,
>>> current period 2,
>,-0.00023394171976718375,
episode 150: ,
overall reward tensor(-0.1026),
overall switching reward tensor(-0.1018),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027616237203320317,
>>> current period 2,
>,-0.0002638542321065258,
episode 151: ,
overall reward tensor(-0.0961),
overall switching reward tensor(-0.0953),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00026363254337740305,
>>> current period 2,
>,-0.0002747034042110951,
episode 152: ,
overall reward tensor(-0.0930),
overall switching reward tensor(-0.0922),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00029014689115592073,
>>> current period 2,
>,-0.00023292668052753124,
episode 153: ,
overall reward tensor(-0.0961),
overall switching reward tensor(-0.0953),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002462459584919933,
>>> current period 2,
>,-0.0002582356469468738,
episode 154: ,
overall reward tensor(-0.0996),
overall switching reward tensor(-0.0988),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000280620550125762,
>>> current period 2,
>,-0.00022960113788633702,
episode 155: ,
overall reward tensor(-0.0890),
overall switching reward tensor(-0.0882),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003064911132397575,
>>> current period 2,
>,-0.00025233397232107737,
episode 156: ,
overall reward tensor(-0.0966),
overall switching reward tensor(-0.0957),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00030488777177460174,
>>> current period 2,
>,-0.00018105859736361018,
episode 157: ,
overall reward tensor(-0.0916),
overall switching reward tensor(-0.0908),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002364209509210765,
>>> current period 2,
>,-0.00027096178324315947,
episode 158: ,
overall reward tensor(-0.1012),
overall switching reward tensor(-0.1004),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002454722447826179,
>>> current period 2,
>,-0.0003056865265276656,
episode 159: ,
overall reward tensor(-0.0953),
overall switching reward tensor(-0.0944),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00033594092665678604,
>>> current period 2,
>,-0.00029460972278869325,
episode 160: ,
overall reward tensor(-0.0968),
overall switching reward tensor(-0.0959),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003069903320900984,
>>> current period 2,
>,-0.00027474019772324315,
episode 161: ,
overall reward tensor(-0.0956),
overall switching reward tensor(-0.0947),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002843909177587526,
>>> current period 2,
>,-0.00029612715028893514,
episode 162: ,
overall reward tensor(-0.0962),
overall switching reward tensor(-0.0953),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028631017679973345,
>>> current period 2,
>,-0.00026805419152145055,
episode 163: ,
overall reward tensor(-0.0946),
overall switching reward tensor(-0.0937),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00029772166735764487,
>>> current period 2,
>,-0.0002116664130001922,
episode 164: ,
overall reward tensor(-0.0989),
overall switching reward tensor(-0.0981),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00023970259674656086,
>>> current period 2,
>,-0.0003104392719249269,
episode 165: ,
overall reward tensor(-0.0946),
overall switching reward tensor(-0.0937),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00025558126942549407,
>>> current period 2,
>,-0.00020614178766104448,
episode 166: ,
overall reward tensor(-0.0954),
overall switching reward tensor(-0.0946),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00014640434118677958,
>>> current period 2,
>,-0.0002462543194069167,
episode 167: ,
overall reward tensor(-0.0711),
overall switching reward tensor(-0.0704),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00025885300118896675,
>>> current period 2,
>,-0.0002444203155116049,
episode 168: ,
overall reward tensor(-0.0749),
overall switching reward tensor(-0.0741),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002745161299500633,
>>> current period 2,
>,-0.00025498970149112767,
episode 169: ,
overall reward tensor(-0.0928),
overall switching reward tensor(-0.0920),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028727114471974684,
>>> current period 2,
>,-0.00028702697647515856,
episode 170: ,
overall reward tensor(-0.0853),
overall switching reward tensor(-0.0844),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002951294779393711,
>>> current period 2,
>,-0.00025125823391101667,
episode 171: ,
overall reward tensor(-0.0949),
overall switching reward tensor(-0.0940),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028306502655107144,
>>> current period 2,
>,-0.00020398941342198793,
episode 172: ,
overall reward tensor(-0.0993),
overall switching reward tensor(-0.0985),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028905691110387284,
>>> current period 2,
>,-0.00019363437441025815,
episode 173: ,
overall reward tensor(-0.0922),
overall switching reward tensor(-0.0915),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00024477621909237574,
>>> current period 2,
>,-0.0001809762607804828,
episode 174: ,
overall reward tensor(-0.0858),
overall switching reward tensor(-0.0851),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00027728495900881137,
>>> current period 2,
>,-0.0002590267764792856,
episode 175: ,
overall reward tensor(-0.0961),
overall switching reward tensor(-0.0953),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00023344959881264608,
>>> current period 2,
>,-0.00022259253126429327,
episode 176: ,
overall reward tensor(-0.0820),
overall switching reward tensor(-0.0812),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00024526913284646327,
>>> current period 2,
>,-0.00028570217878127915,
episode 177: ,
overall reward tensor(-0.0980),
overall switching reward tensor(-0.0972),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00024910661990327045,
>>> current period 2,
>,-0.0002709669938954519,
episode 178: ,
overall reward tensor(-0.0970),
overall switching reward tensor(-0.0962),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002803701488497275,
>>> current period 2,
>,-0.00027023405902914453,
episode 179: ,
overall reward tensor(-0.1016),
overall switching reward tensor(-0.1007),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00022019518601113178,
>>> current period 2,
>,-0.00027796210459362963,
episode 180: ,
overall reward tensor(-0.0870),
overall switching reward tensor(-0.0862),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00020476676176350332,
>>> current period 2,
>,-0.0002480160869627823,
episode 181: ,
overall reward tensor(-0.0852),
overall switching reward tensor(-0.0844),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00022646167798588663,
>>> current period 2,
>,-0.000257391859293509,
episode 182: ,
overall reward tensor(-0.0927),
overall switching reward tensor(-0.0920),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00023415900407882756,
>>> current period 2,
>,-0.0002747697218396424,
episode 183: ,
overall reward tensor(-0.0858),
overall switching reward tensor(-0.0850),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00029505826102689363,
>>> current period 2,
>,-0.0002448291940712027,
episode 184: ,
overall reward tensor(-0.0998),
overall switching reward tensor(-0.0990),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00028448378147000414,
>>> current period 2,
>,-0.00030055396693235754,
episode 185: ,
overall reward tensor(-0.1020),
overall switching reward tensor(-0.1011),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002761577439031685,
>>> current period 2,
>,-0.00026277785415670983,
episode 186: ,
overall reward tensor(-0.0930),
overall switching reward tensor(-0.0921),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002612502047359157,
>>> current period 2,
>,-0.00028133583243638807,
episode 187: ,
overall reward tensor(-0.1004),
overall switching reward tensor(-0.0995),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00024452607531444013,
>>> current period 2,
>,-0.00027324082482901304,
episode 188: ,
overall reward tensor(-0.0966),
overall switching reward tensor(-0.0958),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0003183687703261965,
>>> current period 2,
>,-0.00021910207497811146,
episode 189: ,
overall reward tensor(-0.1033),
overall switching reward tensor(-0.1025),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002679139348462788,
>>> current period 2,
>,-0.0002894911367328446,
episode 190: ,
overall reward tensor(-0.1024),
overall switching reward tensor(-0.1015),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002733177083877318,
>>> current period 2,
>,-0.0003006489036080265,
episode 191: ,
overall reward tensor(-0.0997),
overall switching reward tensor(-0.0988),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002615624566799883,
>>> current period 2,
>,-0.00026896960993383063,
episode 192: ,
overall reward tensor(-0.0982),
overall switching reward tensor(-0.0974),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00024739340078918677,
>>> current period 2,
>,-0.00025560798974483945,
episode 193: ,
overall reward tensor(-0.0946),
overall switching reward tensor(-0.0939),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00030208237305439376,
>>> current period 2,
>,-0.0002558958058725568,
episode 194: ,
overall reward tensor(-0.0925),
overall switching reward tensor(-0.0916),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002093590556758845,
>>> current period 2,
>,-0.0001946906294010351,
episode 195: ,
overall reward tensor(-0.0819),
overall switching reward tensor(-0.0812),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002835719772493873,
>>> current period 2,
>,-0.00025075685833888725,
episode 196: ,
overall reward tensor(-0.0843),
overall switching reward tensor(-0.0835),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0002787215173304014,
>>> current period 2,
>,-0.00027926348622636345,
episode 197: ,
overall reward tensor(-0.0984),
overall switching reward tensor(-0.0976),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00024759501538027505,
>>> current period 2,
>,-0.00029930860150844193,
episode 198: ,
overall reward tensor(-0.0969),
overall switching reward tensor(-0.0961),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00013206281316641787,
>>> current period 2,
>,-0.0003136237689068811,
episode 199: ,
overall reward tensor(-0.0817),
overall switching reward tensor(-0.0809),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.0001566396949152219,
>>> current period 2,
>,-0.00020593861971705406,
episode 200: ,
overall reward tensor(-0.0785),
overall switching reward tensor(-0.0779),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00021049336237516296,
>>> current period 2,
>,-0.00016059131640944313,
episode 201: ,
overall reward tensor(-0.0820),
overall switching reward tensor(-0.0814),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.00020997656565349243,
>>> current period 2,
>,-0.0002694275673752892,
episode 202: ,
overall reward tensor(-0.0988),
overall switching reward tensor(-0.0981),
>>> current period 0,
>,-0.000286848721177568,
>>> current period 1,
>,-0.000187559265319562,
>>> current period 2,
